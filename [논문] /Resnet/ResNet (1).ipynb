{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **ResNet(2015)**"
      ],
      "metadata": {
        "id": "ITrcuabsNfhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abstract\n",
        "* 모델이 깊어질수록 학습하는 것이 어려움\n",
        "* Residual Learning Framework를 통해 모델의 깊이가 깊어져도 정확도 상승\n",
        "* 152개의 Layer를 쌓았음에도 VGG보다 복잡도가 낮았고 ImageNet Dataset에 대해 3.57% Error를 달성\n"
      ],
      "metadata": {
        "id": "QhKaUC5-NuO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Introduction  \n",
        "딥러닝에서 neural networks가 깊어질수록 성능은 더 좋아지지만 train이 어렵다는 한계가 있음  \n",
        "두 가지 문제점  \n",
        "1. Convergence Problem (수렴 문제)\n",
        "2. Degradation Problem (기울기 문제)\n",
        "\n",
        "위와 같은 문제를 해결하기 위해 이 논문은 residual를 이용한 잔차학습(residual learning framework)를 이용해 깊은 신경망에서도 training이 쉽게 이루어질 수 있음을 증명  \n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1xw_jdodllPZq3ky5RobTgDJnwdg-JevD\" height = 200 width = 500>  \n",
        "  \n",
        "Resnet은 수학적으로 어려운 개념이 적용되었다기보다는 방법론적으로 신박한 개념이 도입됨. 이것이 Residual이라는 개념  \n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=18PsIk7zAwSv9IIwbSPlYjh5IujU2m5UT\" height = 200 width = 500>  \n"
      ],
      "metadata": {
        "id": "2g8c5fC9UEkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Residual Learning  \n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=115y8N9Ig8f36ZVjmzY7cLGG5pxWVm-IG\" height = 200 width = 500>  \n",
        "\n",
        "VGG 네트워크는 작은 크기의 3x3 컨볼루션 필터(filter)를 이용해 레이어의 깊이를 늘려 우수한 성능을 보임  \n",
        "하지만 단순히 레이어만 깊게한다고 해서 성능이 좋아지는 건 아님  \n",
        "parameter의 수가 여전히 많았기 때문에 이를 해결하기 위한 다른 접근법 필요   \n",
        "\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1zHx9IV4NggNGuNjULJjN5tRVIxHvqrkb\" height = 200 width = 500>  \n",
        "\n",
        "* x : 입력값  \n",
        "\n",
        "잔여블록(residual block)을 이용해 네트워크의 최적화(optimization)난이도를 낮춤\n",
        "* 실제로 내재한 mapping인 H(x)를 곧바로 학습하는 것은 어려우므로 대신 F(x) = H(x) - x를 학습  \n",
        "* Mapping을 따로 설정해서 이와 같이 H(x)대신 F(x)를 사용  \n",
        "\n",
        "일반적 레이어  \n",
        "*   하나의 입력 -> convolution연산 -> relu -> nonlinear -> 컨볼루션 연산 -> ...\n",
        "*   가중치 값을 개별적으로 학습하기 때문에 수렴 난이도가 높아지는 문제 발생\n",
        "* H(x)가 정답값 y에 정확히 매핑이 되는 함수를 찾는 것을 목적 -> 즉 신경망은 학습을 하면서 H(x) - y의 값을 최소화 시키면서 H(x) = x 가 되는 것을 목표로 함  \n",
        "\n",
        "residual 레이어  \n",
        "*   쉽게하기 위해 input값을 여러번 거친 값들에 x를 더해주는 것으로  끝남  \n",
        "*   앞선 레이어에서 학습된 정보는 그대로 가져옴  \n",
        "*   추가적으로 잔여 정보인 F(x)를 더해주면 끝나서 비교적 단순  \n",
        "*  기존 신경망이 H(x)-x=0을 만들려 했다면 ResNet은 H(x)-x=F(x)로 두어 F(x)를 최소화 시키는게 목적 -> 즉 F(x)=0이라는 목표를 두고 학습 시작, 이렇게 되면 F(x)라는 목표값이 주어지기 때문에 학습이 더 쉬워짐  \n",
        "\n"
      ],
      "metadata": {
        "id": "q-7xjJiLck6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Deep residual Learning  \n"
      ],
      "metadata": {
        "id": "gUvW0J161DYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "수식적으로 접근   \n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1pc8W5RfANa4J4Ep4GgV4HskCspNX1oQa\" height = 200 width = 500>  \n",
        "\n",
        "F함수를 다음과 같이 정의  \n",
        "first weight 값을 곱하도록 만들고 activation function, 여기서는 relu를 사용해서 그 값을 씌워주고 그 다음값을 이어서 곱해주는 형식  \n",
        "추가적으로 x가 더해지는 것을 확인할 수 있음  \n",
        "F값을 정할 때 weight를 단순히 2번만 쓰는게 아니라 더 많은 값을 사용할 수 있기 때문에 이러한 식으로 표현  \n",
        "따라서 multiple convolution layers라 정의  \n",
        "shortcut을 이용해 기존의 입력 값을 그대로 가져와서 그 결과에다가 더해줄 수 있도록 만든다\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8x91HjTuRl_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?id=1BvEoY_q3WRCTo_jEZyeslznpJMeU5LUn\" height = 100 width = 400>  \n",
        "\n",
        "residual이 성능이 더 좋고,  residual 중에서도 깊이가 깊은 net이 성능이 더 좋음"
      ],
      "metadata": {
        "id": "JzSK_Oy94N4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 정리\n",
        "resnet\n",
        "\n",
        "\n",
        "*   기존의 VGG 네트워크보다 더 깊지만 residual block을 활용해 복잡도와 성능 개선\n",
        "*   구현이 매우 간단하며, 학습 난이도가 매우 낮아짐  \n",
        "*   깊이가 깊어질수록 높은 정확도 향상을 보임  \n",
        "\n"
      ],
      "metadata": {
        "id": "ChEST-0v5EVc"
      }
    }
  ]
}